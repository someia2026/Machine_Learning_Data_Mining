{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to `Scikit-learn`\n",
    "\n",
    "Signification des √©moticones :\n",
    "- üåû : documentations importantes\n",
    "- üëÄ : documentations int√©ressantes √† conna√Ætre\n",
    "- üåö : en compl√©ment\n",
    "- (vide) : √† vous de voir\n",
    "\n",
    "-------\n",
    "\n",
    "üåû To introduce this notebook, you can watch this great video of Machine learnia : https://www.youtube.com/watch?v=P6kSc3qVph0 \n",
    "\n",
    "üëÄ And the book linked, from p22 to p35 : https://d1yei2z3i6k35z.cloudfront.net/7374680/66c8a6b7a7f2c_ApprendreleMachineLearningenUneSemaine.pdf?sc=283791024268389748010232f86a5a2297b433793\n",
    "\n",
    "-------\n",
    "\n",
    "The `scikit-learn` package is an open-source library that provides a robust set of machine learning algorithms for Python. It is built upon the core Python scientific stack (*i.e.* NumPy, SciPy, Cython), and has a simple, consistent API, making it useful for a wide range of statistical learning applications.\n",
    "\n",
    "<img src=\"http://1.bp.blogspot.com/-ME24ePzpzIM/UQLWTwurfXI/AAAAAAAAANw/W3EETIroA80/s1600/drop_shadows_background.png\" width=\"800px\"/>\n",
    "\n",
    "## What is Machine Learning?\n",
    "\n",
    "Machine Learning (ML) is about coding programs that automatically adjust their performance from exposure to information encoded in data. This learning is achieved via **tunable parameters** that are automatically adjusted according to performance criteria.\n",
    "\n",
    "Machine Learning can be considered a subfield of Artificial Intelligence (AI).\n",
    "\n",
    "There are three major classes of ML:\n",
    "\n",
    "**Supervised learning**\n",
    ": Algorithms which learn from a training set of *labeled* examples (exemplars) to generalize to the set of all possible inputs. Examples of supervised learning include regression and support vector machines.\n",
    "\n",
    "**Unsupervised learning**\n",
    ": Algorithms which learn from a training set of *unlableled* examples, using the features of the inputs to categorize inputs together according to some statistical criteria. Examples of unsupervised learning include k-means clustering and kernel density estimation.\n",
    "\n",
    "**Reinforcement learning**\n",
    ": Algorithms that learn via reinforcement from a *critic* that provides information on the quality of a solution, but not on how to improve it. Improved solutions are achieved by iteratively exploring the solution space. We will not cover RL in this course.\n",
    "\n",
    "## Representing Data in `scikit-learn`\n",
    "\n",
    "Most machine learning algorithms implemented in scikit-learn expect data to be stored in a\n",
    "**two-dimensional array or matrix**.  The arrays can be\n",
    "either ``numpy`` arrays, or in some cases ``scipy.sparse`` matrices.\n",
    "The size of the array is expected to be `[n_samples, n_features]`\n",
    "\n",
    "- **n_samples:**   The number of samples: each sample is an item to process (e.g. classify).\n",
    "  A sample can be a document, a picture, a sound, a video, an astronomical object,\n",
    "  a row in database or CSV file,\n",
    "  or whatever you can describe with a fixed set of quantitative traits.\n",
    "- **n_features:**  The number of features or distinct traits that can be used to describe each\n",
    "  item in a quantitative manner.  Features are generally real-valued, but may be boolean or\n",
    "  discrete-valued in some cases.\n",
    "\n",
    "The number of features must be fixed in advance. However it can be very high dimensional\n",
    "(e.g. millions of features) with most of them being zeros for a given sample. This is a case\n",
    "where `scipy.sparse` matrices can be useful, in that they are\n",
    "much more memory-efficient than numpy arrays.\n",
    "\n",
    "## Example: Iris morphometrics data\n",
    "\n",
    "One of the datasets included with `scikit-learn` is a set of measurements for flowers, each being a member of one of three species: *Iris Setosa*, *Iris Versicolor* or *Iris Virginica*. \n",
    "\n",
    "<img src=\"http://d.pr/i/1dfvU+\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction √† `Scikit-learn`\n",
    "\n",
    "Signification des √©motic√¥nes :\n",
    "- üåû : documentations importantes\n",
    "- üëÄ : documentations int√©ressantes √† conna√Ætre\n",
    "- üåö : en compl√©ment\n",
    "- (vide) : √† vous de voir\n",
    "\n",
    "-------\n",
    "\n",
    "üåû Pour vous pr√©senter ce cahier, vous pouvez regarder cette excellente vid√©o de Machine learnia : https://www.youtube.com/watch?v=P6kSc3qVph0 \n",
    "\n",
    "üëÄ Et le livre associ√©, de la page 22 √† la page 35 : https://d1yei2z3i6k35z.cloudfront.net/ 7374680/66c8a6b7a7f2c_ApprendreleMachineLearningenUneSemaine.pdf?sc=283791024268389748010232f86a5a2297b433793\n",
    "\n",
    "-------\n",
    "\n",
    "Le package `scikit-learn` est une biblioth√®que open source qui fournit un ensemble robuste d'algorithmes d'apprentissage automatique pour Python. Il s'appuie sur la pile scientifique Python de base (*c'est-√†-dire* NumPy, SciPy, Cython) et dispose d'une API simple et coh√©rente, ce qui le rend utile pour un large √©ventail d'applications d'apprentissage statistique.\n",
    "\n",
    "<img src=\"http://1.bp.blogspot.com/-ME24ePzpzIM/UQLWTwurfXI/AAAAAAAAANw/W3EETIroA80/s1600/drop_shadows_background.png\" width=\"800px\"/>\n",
    "\n",
    "## Qu'est-ce que l'apprentissage automatique ?\n",
    "\n",
    "L'apprentissage automatique (ML) consiste √† coder des programmes qui ajustent automatiquement leurs performances √† partir de l'exposition √† des informations cod√©es dans des donn√©es. Cet apprentissage est r√©alis√© via des **param√®tres r√©glables** qui sont automatiquement ajust√©s en fonction de crit√®res de performance.\n",
    "\n",
    "L'apprentissage automatique peut √™tre consid√©r√© comme un sous-domaine de l'intelligence artificielle (IA).\n",
    "\n",
    "Il existe trois grandes cat√©gories d'apprentissage automatique :\n",
    "\n",
    "**Apprentissage supervis√©**\n",
    ": algorithmes qui apprennent √† partir d'un ensemble d'exemples *√©tiquet√©s* (exemplaires) afin de g√©n√©raliser √† l'ensemble de toutes les entr√©es possibles. Les exemples d'apprentissage supervis√© comprennent la r√©gression et les machines √† vecteurs de support.\n",
    "\n",
    "**Apprentissage non supervis√©**\n",
    ": algorithmes qui apprennent √† partir d'un ensemble d'exemples *non √©tiquet√©s*, en utilisant les caract√©ristiques des entr√©es pour les classer selon certains crit√®res statistiques. Parmi les exemples d'apprentissage non supervis√©, on peut citer le clustering k-means et l'estimation de densit√© du noyau.\n",
    "\n",
    "**Apprentissage par renforcement**\n",
    ": algorithmes qui apprennent par renforcement √† partir d'un *critique* qui fournit des informations sur la qualit√© d'une solution, mais pas sur la mani√®re de l'am√©liorer. Les solutions am√©lior√©es sont obtenues en explorant de mani√®re it√©rative l'espace des solutions. Nous ne traiterons pas l'apprentissage par renforcement dans ce cours.\n",
    "\n",
    "## Repr√©sentation des donn√©es dans `scikit-learn`\n",
    "\n",
    "La plupart des algorithmes d'apprentissage automatique impl√©ment√©s dans scikit-learn s'attendent √† ce que les donn√©es soient stock√©es dans un\n",
    "**tableau ou une matrice bidimensionnelle**.  Les tableaux peuvent √™tre\n",
    "soit des tableaux ``numpy``, soit, dans certains cas, des matrices ``scipy.sparse``.\n",
    "La taille du tableau doit √™tre `[n_samples, n_features]`\n",
    "\n",
    "- **n_samples :**   Le nombre d'√©chantillons : chaque √©chantillon est un √©l√©ment √† traiter (par exemple, √† classer).\n",
    "  Un √©chantillon peut √™tre un document, une image, un son, une vid√©o, un objet astronomique,\n",
    "  une ligne dans une base de donn√©es ou un fichier CSV,\n",
    "  ou tout ce que vous pouvez d√©crire √† l'aide d'un ensemble fixe de caract√©ristiques quantitatives.\n",
    "- **n_features :**  Le nombre de caract√©ristiques ou de traits distinctifs pouvant √™tre utilis√©s pour d√©crire chaque\n",
    "  √©l√©ment de mani√®re quantitative.  Les caract√©ristiques sont g√©n√©ralement des valeurs r√©elles, mais peuvent √™tre bool√©ennes ou\n",
    "  discr√®tes dans certains cas.\n",
    "\n",
    "Le nombre de caract√©ristiques doit √™tre fix√© √† l'avance. Cependant, il peut √™tre tr√®s √©lev√©\n",
    "(par exemple, des millions de caract√©ristiques), la plupart d'entre elles √©tant des z√©ros pour un √©chantillon donn√©. C'est un cas\n",
    "o√π les matrices `scipy.sparse` peuvent √™tre utiles, car elles sont\n",
    "beaucoup plus efficaces en termes de m√©moire que les tableaux numpy.\n",
    "\n",
    "## Exemple : donn√©es morphom√©triques sur les iris\n",
    "\n",
    "L'un des ensembles de donn√©es inclus dans `scikit-learn` est un ensemble de mesures de fleurs, chacune appartenant √† l'une des trois esp√®ces suivantes : *Iris Setosa*, *Iris Versicolor* ou *Iris Virginica*. \n",
    "\n",
    "<img src=\"http://d.pr/i/1dfvU+\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris_data = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `load_iris` function returns the components of the iris dataset in a Python dictionary. We can see the labels for the various components by examining the dict `keys`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features (variables) of the iris data consist of:\n",
    "\n",
    "  1. sepal length in cm\n",
    "  2. sepal width in cm\n",
    "  3. petal length in cm\n",
    "  4. petal width in cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_data['data'][:10] # pyright: ignore[reportUndefinedVariable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples, n_features = iris.data.shape\n",
    "n_samples, n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.1, 3.5, 1.4, 0.2])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of problems know as *classification problems* try to predict to which group and example belongs. The iris data set is a suitable example of a supervised classification problem, as there are class labels for each example in the data.\n",
    "\n",
    "The information about the class of each sample is stored in the ``target`` attribute of the dataset. They inlude the following species:\n",
    "\n",
    "  1. Iris Setosa\n",
    "  2. Iris Versicolour\n",
    "  3. Iris Virginica\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are encoded as `0,1,2`, respectively, in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([50, 50, 50])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.bincount(iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "iris_data = pd.DataFrame(iris.data, columns=iris.feature_names).assign(species=iris.target_names[iris.target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "sns.pairplot(iris_data, hue='species', height=1.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `scikit-learn` interface\n",
    "\n",
    "All objects within scikit-learn share a uniform common basic API consisting of three complementary interfaces: \n",
    "\n",
    "* **estimator** interface for building and Ô¨Åtting models\n",
    "* **predictor** interface for making predictions\n",
    "* **transformer** interface for converting data.\n",
    "\n",
    "The estimator interface is at the core of the library. It deÔ¨Ånes instantiation mechanisms of objects and exposes a fit method for learning a model from training data. All supervised and unsupervised learning algorithms (*e.g.*, for classiÔ¨Åcation, regression or clustering) are oÔ¨Äered as objects implementing this interface. Machine learning tasks like feature extraction, feature selection or dimensionality reduction are also provided as estimators.\n",
    "\n",
    "Scikit-learn strives to have a uniform interface across all methods. For example, a typical **estimator** follows this template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator(object):\n",
    "  \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit model to data X (and y)\"\"\"\n",
    "        self.some_attribute = self.some_fitting_method(X, y)\n",
    "        return self\n",
    "            \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Make prediction based on passed features\"\"\"\n",
    "        pred = self.make_prediction(X_test)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given scikit-learn **estimator** object named `model`, several methods are available. Irrespective of the type of **estimator**, there will be a `fit` method:\n",
    "\n",
    "- `model.fit` : fit training data. For supervised learning applications, this accepts two arguments: the data `X` and the labels `y` (e.g. `model.fit(X, y)`). For unsupervised learning applications, this accepts only a single argument, the data `X` (e.g. `model.fit(X)`).\n",
    "\n",
    "> During the fitting process, the state of the **estimator** is stored in attributes of the estimator instance named with a trailing underscore character (\\_). For example, the sequence of regression trees `sklearn.tree.DecisionTreeRegressor` is stored in `estimators_` attribute.\n",
    "\n",
    "The **predictor** interface extends the notion of an estimator by adding a `predict` method that takes an array `X_test` and produces predictions based on the learned parameters of the estimator. In the case of supervised learning estimators, this method typically returns the predicted labels or values computed by the model. Some unsupervised learning estimators may also implement the predict interface, such as k-means, where the predicted values are the cluster labels.\n",
    "\n",
    "**supervised estimators** are expected to have the following methods:\n",
    "\n",
    "- `model.predict` : given a trained model, predict the label of a new set of data. This method accepts one argument, the new data `X_new` (e.g. `model.predict(X_new)`), and returns the learned label for each object in the array.\n",
    "- `model.predict_proba` : For classification problems, some estimators also provide this method, which returns the probability that a new observation has each categorical label. In this case, the label with the highest probability is returned by `model.predict()`.\n",
    "- `model.score` : for classification or regression problems, most (all?) estimators implement a score method.  Scores are between 0 and 1, with a larger score indicating a better fit.\n",
    "\n",
    "Since it is common to modify or Ô¨Ålter data before feeding it to a learning algorithm, some estimators in the library implement a **transformer** interface which deÔ¨Ånes a `transform` method. It takes as input some new data `X_test` and yields as output a transformed version. Preprocessing, feature selection, feature extraction and dimensionality reduction algorithms are all provided as transformers within the library.\n",
    "\n",
    "**unsupervised estimators** will always have these methods:\n",
    "\n",
    "- `model.transform` : given an unsupervised model, transform new data into the new basis. This also accepts one argument `X_new`, and returns the new representation of the data based on the unsupervised model.\n",
    "- `model.fit_transform` : some estimators implement this method, which more efficiently performs a fit and a transform on the same input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* üëÄ [`scikit-learn` user's guide](http://scikit-learn.org/stable/user_guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Bonus] - Example: Logistic Regression Analysis\n",
    "\n",
    "To demonstrate how `scikit-learn` and it's interface is used, let's conduct a logistic regression analysis on a dataset for very low birth weight (VLBW) infants. Logistic regression is a type of analysis that can be used to predict labels or classes (rather than a continuous variable, like when we use normal *linear regression*).\n",
    "\n",
    "Data on 671 infants with very low (less than 1600 grams) birth weight from 1981-87 were collected at Duke University Medical Center by [OShea *et al.* (1992)](http://www.ncbi.nlm.nih.gov/pubmed/1635885). Of interest is the relationship between the outcome **intra-ventricular hemorrhage** and the predictors birth weight, gestational age, presence of pneumothorax, mode of delivery, single vs. multiple birth, and whether the birth occurred at Duke or at another hospital with later transfer to Duke. A secular trend in the outcome is also of interest.\n",
    "\n",
    "https://biostat.app.vumc.org/wiki/pub/Main/DataSets/Cvlbw.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\somei\\AppData\\Local\\Temp\\ipykernel_22752\\4110043920.py:9: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  y = subset.ivh.replace({'absent':0, 'possible':1, 'definite':1}) # 0 no ivh issue, 1 possible or definite issue\n",
      "C:\\Users\\somei\\AppData\\Local\\Temp\\ipykernel_22752\\4110043920.py:16: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X0['csection'] = subset.delivery.replace({'vaginal':0, 'abdominal':1})\n",
      "C:\\Users\\somei\\AppData\\Local\\Temp\\ipykernel_22752\\4110043920.py:17: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X0['transported'] = subset.inout.replace({'born at Duke':0, 'transported':1})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gest</th>\n",
       "      <th>bwt</th>\n",
       "      <th>pltct</th>\n",
       "      <th>lowph</th>\n",
       "      <th>csection</th>\n",
       "      <th>transported</th>\n",
       "      <th>pneumo</th>\n",
       "      <th>twn</th>\n",
       "      <th>apg1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.381853</td>\n",
       "      <td>-0.640950</td>\n",
       "      <td>-1.756012</td>\n",
       "      <td>-1.669742</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.381853</td>\n",
       "      <td>0.092756</td>\n",
       "      <td>-0.174208</td>\n",
       "      <td>-0.114757</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.381853</td>\n",
       "      <td>0.370375</td>\n",
       "      <td>2.037845</td>\n",
       "      <td>1.662393</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1.236762</td>\n",
       "      <td>-1.255677</td>\n",
       "      <td>0.727914</td>\n",
       "      <td>0.329542</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.755419</td>\n",
       "      <td>1.599828</td>\n",
       "      <td>-0.124777</td>\n",
       "      <td>-0.484986</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        gest       bwt     pltct     lowph  csection  transported  pneumo  \\\n",
       "5  -0.381853 -0.640950 -1.756012 -1.669742         1            0     1.0   \n",
       "13 -0.381853  0.092756 -0.174208 -0.114757         0            0     1.0   \n",
       "14 -0.381853  0.370375  2.037845  1.662393         1            0     0.0   \n",
       "16 -1.236762 -1.255677  0.727914  0.329542         0            0     0.0   \n",
       "17  1.755419  1.599828 -0.124777 -0.484986         1            0     0.0   \n",
       "\n",
       "    twn  apg1  \n",
       "5   0.0   5.0  \n",
       "13  0.0   6.0  \n",
       "14  0.0   6.0  \n",
       "16  0.0   4.0  \n",
       "17  0.0   8.0  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "vlbw = pd.read_csv(\"./data/vlbw.csv\", index_col=0)\n",
    "\n",
    "subset = vlbw[['ivh', 'gest', 'bwt', 'delivery', 'inout', \n",
    "               'pltct', 'lowph', 'pneumo', 'twn', 'apg1']].dropna()\n",
    "\n",
    "# Extract response variable\n",
    "y = subset.ivh.replace({'absent':0, 'possible':1, 'definite':1}) # 0 no ivh issue, 1 possible or definite issue\n",
    "\n",
    "# Standardize some variables\n",
    "X = subset[['gest', 'bwt', 'pltct', 'lowph']]\n",
    "X0 = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "# Recode some variables\n",
    "X0['csection'] = subset.delivery.replace({'vaginal':0, 'abdominal':1})\n",
    "X0['transported'] = subset.inout.replace({'born at Duke':0, 'transported':1})\n",
    "X0[['pneumo', 'twn', 'apg1']] = subset[['pneumo', 'twn','apg1']]\n",
    "X0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['birth', 'exit', 'hospstay', 'lowph', 'pltct', 'race', 'bwt', 'gest',\n",
       "       'inout', 'twn', 'lol', 'magsulf', 'meth', 'toc', 'delivery', 'apg1',\n",
       "       'vent', 'pneumo', 'pda', 'cld', 'pvh', 'ivh', 'ipe', 'year', 'sex',\n",
       "       'dead'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vlbw.columns # pyright: ignore[reportUndefinedVariable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = y.unique() # pyright: ignore[reportUndefinedVariable]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data into a training set and a testing set. By default, 25% of the data is reserved for testing. This is the first of multiple ways that we will see to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X0, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X_train: (348, 9)\n",
      "Shape X_test: (116, 9)\n"
     ]
    }
   ],
   "source": [
    "print('Shape X_train:', X_train.shape)\n",
    "print('Shape X_test:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LogisticRegression` model in scikit-learn employs a regularization coefficient `C`, which defaults to 1. The amount of regularization is lower with larger values of C.\n",
    "\n",
    "Regularization penalizes the values of regression coefficients, while smaller ones let the coefficients range widely. Scikit-learn includes two penalties: a **l2** penalty which penalizes the sum of the squares of the coefficients (the default), and a **l1** penalty which penalizes the sum of the absolute values.\n",
    "\n",
    "Why do regularization:\n",
    "* The objectif is to prevent the algorithm from overfitting.\n",
    "* Regularization is any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error. (Ian Goodfellow)\n",
    "* Estimated coefficients are shrunken towards zero. This shrinkage has the effect of reducing variance, thus generalizing better on unseen data. (ISL page 204)\n",
    "\n",
    "However, when the shrinkage or regularization paramters are chosen to large, the model starts loosing some properties (i.e. some coefficients might be shrunken to much) which can lead to a biased model. The parameters needs thus be chosen carefully.\n",
    "\n",
    "Further reading\n",
    "* üëÄ https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a \n",
    "* üåö https://www.knime.com/blog/regularization-for-logistic-regression-l1-l2-gauss-or-laplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lrmod = LogisticRegression(C=1000)\n",
    "lrmod.fit(X_train, y_train)\n",
    "\n",
    "pred_train = lrmod.predict(X_train)\n",
    "pred_test = lrmod.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix is used to evaluate the performance of a classification algorithm. In this context positive is to be interpreted as : possible or definite ivh (class 1).\n",
    "\n",
    "<img src='./images/confusion-matrix.png' width=600px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x1cca7623230>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_train, pred_train, labels=classes)\n",
    "_ = disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8753799392097265"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "288/329"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "288 of 329 births are predicted class 0: absent IVH. This is 88%. At first site we might conclude that the model performs fairly well. However, we're interested in how well the model is in predicting class 1, possible or definite IVH. Now suddenly the model performs less weel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision train set: 68.0 %\n"
     ]
    }
   ],
   "source": [
    "# Precision = TP / (TP + FP)\n",
    "precision = 13/(6+13)\n",
    "print('Precision train set:', round(precision, 2)*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x1ccb048bb10>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_test, pred_test, labels=classes)\n",
    "_ = disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8715596330275229"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "95/109"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision test set: 43.0 %\n"
     ]
    }
   ],
   "source": [
    "# Precision of model on previsouly unseen data\n",
    "\n",
    "precision = 3 /7\n",
    "print('precision test set:', round(precision, 2)*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the test set the data is even worse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can bootstrap some confidence intervals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = 1000\n",
    "boot_samples = np.empty((n, len(lrmod.coef_[0])))\n",
    "\n",
    "for i in np.arange(n):\n",
    "    boot_ind = np.random.randint(0, len(X0), len(X0))\n",
    "    y_i, X_i = y.values[boot_ind], X0.values[boot_ind]\n",
    "    \n",
    "    lrmod_i = LogisticRegression(C=1000)\n",
    "    lrmod_i.fit(X_i, y_i)\n",
    "\n",
    "    boot_samples[i] = lrmod_i.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "boot_samples.sort(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "boot_se = boot_samples[[25, 975], :].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x1ccb04efed0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "coefs = lrmod.coef_[0]\n",
    "plt.plot(coefs, 'r.')\n",
    "for i in range(len(coefs)):\n",
    "    plt.errorbar(x=[i,i], y=boot_se[i], color='red')\n",
    "plt.xlim(-0.5, 8.5)\n",
    "plt.xticks(range(len(coefs)), X0.columns.values, rotation=45)\n",
    "plt.axhline(0, color='k', linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
